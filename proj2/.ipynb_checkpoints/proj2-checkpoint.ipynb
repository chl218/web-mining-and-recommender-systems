{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "  for l in urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print(\"done\")\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], datum['review/palate'], datum['review/overall']]\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV'] >= 6.5 for d in data]\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Logistic regression by gradient ascent         #\n",
    "##################################################\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= (1.05*logit)\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= X[i][k]\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return np.array([-x for x in dl])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shuffled:  (50000, 6)\n",
      "y shuffled:  (50000,)\n",
      "x train:  (16667, 6) x validate:  (16666, 6) x test:  (16665, 6)\n",
      "y train:  (16667,) y validate:  (16666,) y test:  (16665,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# split into 1/3 train, 1/3 validation, 1/3 test\n",
    "\n",
    "Z = list(zip(X, y))\n",
    "\n",
    "random.shuffle(Z)\n",
    "\n",
    "x_shuffled, y_shuffled = zip(*Z)\n",
    "\n",
    "print(\"X shuffled: \", np.shape(x_shuffled))\n",
    "print(\"y shuffled: \", np.shape(y_shuffled))\n",
    "\n",
    "samples = len(x_shuffled)\n",
    "\n",
    "X_train = x_shuffled[0:round(samples/3)];\n",
    "y_train = y_shuffled[0:round(samples/3)];\n",
    "\n",
    "X_validation = x_shuffled[round(samples/3) + 1: 2 * round(samples/3)]\n",
    "y_validation = y_shuffled[round(samples/3) + 1: 2 * round(samples/3)]\n",
    "\n",
    "X_test = x_shuffled[2 * round(samples/3) + 1:samples]\n",
    "y_test = y_shuffled[2 * round(samples/3) + 1:samples]\n",
    "\n",
    "print(\"x train: \", np.shape(X_train), \"x validate: \", np.shape(X_validation), \"x test: \", np.shape(X_test))\n",
    "print(\"y train: \", np.shape(y_train), \"y validate: \", np.shape(y_validation), \"y test: \", np.shape(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Train                                          #\n",
    "##################################################\n",
    "\n",
    "def train(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "  return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Predict                                        #\n",
    "##################################################\n",
    "\n",
    "def performance(theta, X, y):\n",
    "  scores = [inner(theta,x) for x in X]\n",
    "  predictions = [s > 0 for s in scores]\n",
    "    \n",
    "  positives = sum(predictions)\n",
    "  negatives = len(predictions) - sum(predictions)\n",
    "    \n",
    "  correct = [(a==b) for (a,b) in zip(predictions, y)]\n",
    "    \n",
    "  truePositives = sum(correct)\n",
    "  trueNegatives = len(correct) - sum(correct)\n",
    "\n",
    "  falsePositives = sum([(a==1 and b==0) for (a,b) in zip(predictions,y)])\n",
    "  falseNegatives = sum([(a==0 and b==1) for (a,b) in zip(predictions,y)])\n",
    " \n",
    "  acc = sum(correct) * 1.0 / len(correct)\n",
    "  return acc, positives, negatives, truePositives, trueNegatives, falsePositives, falseNegatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "lam = 1.0\n",
    "theta = train(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training Set \n",
      "\n",
      "Positives:  13670\n",
      "Negatives:  2997 \n",
      "\n",
      "True Positives:  11200\n",
      "True Positives:  5467 \n",
      "\n",
      "False Positives:  4369\n",
      "False Negatives:  1098 \n",
      "\n",
      "lambda = 1.0:\taccuracy=0.6719865602687947 \n",
      "\n",
      "==================================================\n",
      "Validation Set \n",
      "\n",
      "Positives:  13639\n",
      "Negatives:  3027 \n",
      "\n",
      "True Positives:  11379\n",
      "True Positives:  5287 \n",
      "\n",
      "False Positives:  4302\n",
      "False Negatives:  985 \n",
      "\n",
      "lambda = 1.0:\taccuracy=0.6827673106924277 \n",
      "\n",
      "==================================================\n",
      "Testing Set \n",
      "\n",
      "Positives:  13669\n",
      "Negatives:  2996 \n",
      "\n",
      "True Positives:  11377\n",
      "True Positives:  5288 \n",
      "\n",
      "False Positives:  4255\n",
      "False Negatives:  1033 \n",
      "\n",
      "lambda = 1.0:\taccuracy=0.6826882688268827 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels  = [\"Training Set\", \"Validation Set\", \"Testing Set\"]\n",
    "corpusX = [X_train, X_validation, X_test]\n",
    "corpusY = [y_train, y_validation, y_test]\n",
    "\n",
    "for (label, x, y) in zip(labels, corpusX, corpusY):\n",
    "    print(\"==================================================\")\n",
    "    print(label, \"\\n\")\n",
    "    acc, positives, negatives, truePositives, trueNegatives, falsePositives, falseNegatives = performance(theta, x, y)\n",
    "    \n",
    "    print(\"Positives: \", positives)\n",
    "    print(\"Negatives: \", negatives, \"\\n\")\n",
    "\n",
    "    print(\"True Positives: \", truePositives)\n",
    "    print(\"True Positives: \", trueNegatives, \"\\n\")\n",
    "    \n",
    "    print(\"False Positives: \", falsePositives)\n",
    "    print(\"False Negatives: \", falseNegatives, \"\\n\")\n",
    "    \n",
    "    print(\"lambda = \" + str(lam) + \":\\taccuracy=\" + str(acc), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Training Set \n",
      "\n",
      "Positives:  13670\n",
      "Negatives:  2997 \n",
      "\n",
      "True Positives:  11200\n",
      "True Positives:  5467 \n",
      "\n",
      "False Positives:  4369\n",
      "False Negatives:  1098 \n",
      "\n",
      "lambda = 1.0:\taccuracy=0.6719865602687947 \n",
      "\n",
      "==================================================\n",
      "Validation Set \n",
      "\n",
      "Positives:  13639\n",
      "Negatives:  3027 \n",
      "\n",
      "True Positives:  11379\n",
      "True Positives:  5287 \n",
      "\n",
      "False Positives:  4302\n",
      "False Negatives:  985 \n",
      "\n",
      "lambda = 1.0:\taccuracy=0.6827673106924277 \n",
      "\n",
      "==================================================\n",
      "Testing Set \n",
      "\n",
      "Positives:  13669\n",
      "Negatives:  2996 \n",
      "\n",
      "True Positives:  11377\n",
      "True Positives:  5288 \n",
      "\n",
      "False Positives:  4255\n",
      "False Negatives:  1033 \n",
      "\n",
      "lambda = 1.0:\taccuracy=0.6826882688268827 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (label, x, y) in zip(labels, corpusX, corpusY):\n",
    "    print(\"==================================================\")\n",
    "    print(label, \"\\n\")\n",
    "    acc, positives, negatives, truePositives, trueNegatives, falsePositives, falseNegatives = performance(theta, x, y)\n",
    "    \n",
    "    print(\"Positives: \", positives)\n",
    "    print(\"Negatives: \", negatives, \"\\n\")\n",
    "\n",
    "    print(\"True Positives: \", truePositives)\n",
    "    print(\"True Positives: \", trueNegatives, \"\\n\")\n",
    "    \n",
    "    print(\"False Positives: \", falsePositives)\n",
    "    print(\"False Negatives: \", falseNegatives, \"\\n\")\n",
    "    \n",
    "    print(\"lambda = \" + str(lam) + \":\\taccuracy=\" + str(acc), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = [0, 0.01, 0.1, 1, 100]\n",
    "labels  = [\"Training Set\", \"Validation Set\", \"Testing Set\"]\n",
    "corpusX = [X_train, X_validation, X_test]\n",
    "corpusY = [y_train, y_validation, y_test]\n",
    "\n",
    "acc = []\n",
    "positives = []\n",
    "negatives = []\n",
    "truePositives = []\n",
    "trueNegatives = []\n",
    "falsePositives = []\n",
    "falseNegatives = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    theta = train(lam)\n",
    "    for (x, y) in zip(corpusX, corpusY):\n",
    "        _acc, _positives, _negatives, _truePositives, _trueNegatives, _falsePositives, _falseNegatives \\\n",
    "            = performance(theta, x, y)\n",
    "            \n",
    "        acc.append(_acc)\n",
    "        positives.append(_positives)\n",
    "        negatives.append(_negatives)\n",
    "        truePositives.append(_truePositives)\n",
    "        trueNegatives.append(_trueNegatives)\n",
    "        falsePositives.append(_falsePositives)\n",
    "        falseNegatives.append(_falseNegatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambdas:         0.000 0.000 0.000 0.010 0.010 0.010 0.100 0.100 0.100 1.000 1.000 1.000 100.000 100.000 100.000 \n",
      "Accuracy:        0.671 0.683 0.682 0.671 0.683 0.682 0.671 0.683 0.682 0.672 0.683 0.683 0.663 0.667 0.674 \n",
      "\n",
      "Positives:       13632 13617 13640 13632 13617 13640 13635 13617 13642 13670 13639 13669 14904 14852 14892 \n",
      "Negatives:        3035  3049  3025  3035  3049  3025  3032  3049  3023  2997  3027  2996  1763  1814  1773 \n",
      "\n",
      "True Positives:  11190 11383 11372 11190 11383 11372 11191 11383 11370 11200 11379 11377 11048 11110 11226 \n",
      "True Negatives:   5477  5283  5293  5477  5283  5293  5476  5283  5295  5467  5287  5288  5619  5556  5439 \n",
      "\n",
      "False Positives:  4355  4289  4243  4355  4289  4243  4356  4289  4245  4369  4302  4255  5062  5043  4942 \n",
      "False Negatives:  1122   994  1050  1122   994  1050  1120   994  1050  1098   985  1033   557   513   497 "
     ]
    }
   ],
   "source": [
    "print(\"Lambdas:         \", end='') \n",
    "[print(\"%.3f \" % val, end='') for val in np.repeat(lambdas,3)]\n",
    "print(\"\\nAccuracy:        \", end='') \n",
    "[print(\"%.3f \" % val, end='') for val in acc]\n",
    "\n",
    "print(\"\\n\\nPositives:       \", end='') \n",
    "[print(\"%5d \" % val, end='') for val in positives]\n",
    "print(\"\\nNegatives:       \", end='') \n",
    "[print(\"%5d \" % val, end='') for val in negatives]\n",
    "\n",
    "print(\"\\n\\nTrue Positives:  \", end='') \n",
    "[print(\"%5d \" % val, end='') for val in truePositives]\n",
    "print(\"\\nTrue Negatives:  \", end='') \n",
    "[print(\"%5d \" % val, end='') for val in trueNegatives]\n",
    "\n",
    "print(\"\\n\\nFalse Positives: \", end='') \n",
    "[print(\"%5d \" % val, end='') for val in falsePositives]\n",
    "print(\"\\nFalse Negatives: \", end='') \n",
    "[print(\"%5d \" % val, end='') for val in falseNegatives];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Network visualization ###\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = set()\n",
    "nodes = set()\n",
    "for edge in urlopen(\"http://jmcauley.ucsd.edu/cse255/data/facebook/egonet.txt\"):\n",
    "  x,y = edge.split()\n",
    "  x,y = int(x),int(y)\n",
    "  edges.add((x,y))\n",
    "  edges.add((y,x))\n",
    "  nodes.add(x)\n",
    "  nodes.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFCCAYAAADGwmVOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neqhknOHJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "for e in edges:\n",
    "  G.add_edge(e[0],e[1])\n",
    "nx.draw(G)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Connected Components:  3\n",
      "Largest Connected Component: 40\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Connected Components: \", nx.number_connected_components(G))\n",
    "print(\"Largest Connected Component:\", max([len(x) for x in nx.connected_components(G)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low IDs:   [697, 703, 708, 713, 719, 729, 745, 747, 753, 769, 772, 774, 798, 800, 803, 804, 805, 810, 811, 819]\n",
      "High IDs:  [823, 825, 828, 830, 840, 856, 861, 863, 864, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893]\n",
      "Normalized Cut Cost:  0.4224058769513316\n"
     ]
    }
   ],
   "source": [
    "largest_cc = sorted(max(nx.connected_components(G), key=len))\n",
    "\n",
    "half = round(len(largest_cc)/2)\n",
    "\n",
    "split_lo = largest_cc[:half]\n",
    "split_hi = largest_cc[half:]\n",
    "\n",
    "print(\"Low IDs:  \", split_lo)\n",
    "print(\"High IDs: \", split_hi)\n",
    "print(\"Normalized Cut Cost: \", (1/2)*nx.normalized_cut_size(G, split_lo, split_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697  703  708  713  719  729  745  747  753  769  772  774  798  800  803  804  805  810  811  819  "
     ]
    }
   ],
   "source": [
    "for val in split_lo:\n",
    "    print(val, ' ', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Iterations:  3\n",
      "New Low IDs:   [697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 805, 810, 811, 819, 823, 828, 830, 840, 856, 869, 880, 890]\n",
      "New High IDs:  [729, 804, 825, 861, 863, 864, 876, 878, 882, 884, 886, 888, 889, 893]\n",
      "Normalized Cut Cost:  0.09817045961624274\n"
     ]
    }
   ],
   "source": [
    "prevCost = 0;\n",
    "currCost = (1/2)*nx.normalized_cut_size(G, split_lo, split_hi)\n",
    "\n",
    "iter = 0\n",
    "while prevCost != currCost:\n",
    "    iter += 1\n",
    "    \n",
    "    prevCost = currCost\n",
    "    \n",
    "    lo = [i for i in split_lo] # deep copy\n",
    "    hi = [i for i in split_hi] # deep copy\n",
    "    \n",
    "    for val in split_lo:          \n",
    "        lo.remove(val)\n",
    "        hi.append(val)\n",
    "\n",
    "        newCost = (1/2)*nx.normalized_cut_size(G, lo, hi)\n",
    "            \n",
    "        if(newCost > currCost):\n",
    "            lo.append(val)\n",
    "            hi.remove(val)\n",
    "        else:\n",
    "            currCost = newCost \n",
    "\n",
    "    for val in split_hi:\n",
    "        lo.append(val)\n",
    "        hi.remove(val)\n",
    "        newCost = (1/2)*nx.normalized_cut_size(G, lo, hi)\n",
    "            \n",
    "        if(newCost > currCost):\n",
    "            lo.remove(val)\n",
    "            hi.append(val)\n",
    "        else:\n",
    "            currCost = newCost\n",
    "           \n",
    "    split_lo = [i for i in lo]\n",
    "    split_hi = [i for i in hi]\n",
    "    \n",
    "print(\"\\nNumber of Iterations: \", iter)\n",
    "print(\"New Low IDs:  \", sorted(split_lo))\n",
    "print(\"New High IDs: \", sorted(split_hi))\n",
    "print(\"Normalized Cut Cost: \", (1/2)*nx.normalized_cut_size(G, split_lo, split_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
