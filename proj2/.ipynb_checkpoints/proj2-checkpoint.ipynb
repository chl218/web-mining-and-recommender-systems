{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "  for l in urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print(\"Done\")\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], datum['review/palate'], datum['review/overall']]\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV'] >= 6.5 for d in data]\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Logistic regression by gradient ascent         #\n",
    "##################################################\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= logit\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= X[i][k]\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return np.array([-x for x in dl])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shuffled:  (50000, 6)\n",
      "y shuffled:  (50000,)\n",
      "x train:  (16667, 6) x validate:  (16666, 6) x test:  (16665, 6)\n",
      "y train:  (16667,) y validate:  (16666,) y test:  (16665,)\n"
     ]
    }
   ],
   "source": [
    "# split into 1/3 train, 1/3 validation, 1/3 test\n",
    "\n",
    "Z = list(zip(X, y))\n",
    "\n",
    "random.shuffle(Z)\n",
    "\n",
    "x_shuffled, y_shuffled = zip(*Z)\n",
    "\n",
    "print(\"X shuffled: \", np.shape(x_shuffled))\n",
    "print(\"y shuffled: \", np.shape(y_shuffled))\n",
    "\n",
    "samples = len(x_shuffled)\n",
    "\n",
    "X_train = x_shuffled[0:round(samples/3)];\n",
    "y_train = y_shuffled[0:round(samples/3)];\n",
    "\n",
    "X_validation = x_shuffled[round(samples/3) + 1: 2 * round(samples/3)]\n",
    "y_validation = y_shuffled[round(samples/3) + 1: 2 * round(samples/3)]\n",
    "\n",
    "X_test = x_shuffled[2 * round(samples/3) + 1:samples]\n",
    "y_test = y_shuffled[2 * round(samples/3) + 1:samples]\n",
    "\n",
    "print(\"x train: \", np.shape(X_train), \"x validate: \", np.shape(X_validation), \"x test: \", np.shape(X_test))\n",
    "print(\"y train: \", np.shape(y_train), \"y validate: \", np.shape(y_validation), \"y test: \", np.shape(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Train                                          #\n",
    "##################################################\n",
    "\n",
    "def train(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "  return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Predict                                        #\n",
    "##################################################\n",
    "\n",
    "def performance(theta, X, y):\n",
    "  scores = [inner(theta,x) for x in X]\n",
    "  predictions = [s > 0 for s in scores]\n",
    "    \n",
    "  positives = sum(predictions)\n",
    "  negatives = len(predictions) - sum(predictions)\n",
    "    \n",
    "  correct = [(a==b) for (a,b) in zip(predictions, y)]\n",
    "    \n",
    "  truePositives = sum(correct)\n",
    "  trueNegatives = len(correct) - sum(correct)\n",
    "\n",
    "  falsePositives = sum([(a==1 and b==0) for (a,b) in zip(predictions,y)])\n",
    "  falseNegatives = sum([(a==0 and b==1) for (a,b) in zip(predictions,y)])\n",
    " \n",
    "  acc = sum(correct) * 1.0 / len(correct)\n",
    "  return acc, positives, negatives, truePositives, trueNegatives, falsePositives, falseNegatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "lam = 1.0\n",
    "theta = train(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|   Lambda = 1.0  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7212455750884982 | 0.7137885515420617 | 0.7189318931893189 |\n",
      "|    Positives    |       12463        |       12300        |       12408        |\n",
      "|    Negatives    |        4204        |        4366        |        4257        |\n",
      "|  True Positives |       12021        |       11896        |       11981        |\n",
      "|  True Negatives |        4646        |        4770        |        4684        |\n",
      "| False Positives |        3349        |        3363        |        3339        |\n",
      "| False Negatives |        1297        |        1407        |        1345        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "corpusX = [X_train, X_validation, X_test]\n",
    "corpusY = [y_train, y_validation, y_test]\n",
    "\n",
    "acc = []\n",
    "positives = []\n",
    "negatives = []\n",
    "truePositives = []\n",
    "trueNegatives = []\n",
    "falsePositives = []\n",
    "falseNegatives = []\n",
    "\n",
    "for (x, y) in zip(corpusX, corpusY):\n",
    "    _acc, _positives, _negatives, _truePositives, _trueNegatives, _falsePositives, _falseNegatives \\\n",
    "        = performance(theta, x, y)\n",
    "\n",
    "    acc.append(_acc)\n",
    "    positives.append(_positives)\n",
    "    negatives.append(_negatives)\n",
    "    truePositives.append(_truePositives)\n",
    "    trueNegatives.append(_trueNegatives)\n",
    "    falsePositives.append(_falsePositives)\n",
    "    falseNegatives.append(_falseNegatives)\n",
    "    \n",
    "\n",
    "t = PrettyTable(['Lambda = ' + str(lam), 'Train', 'Validation', 'Test'])\n",
    "t.add_row([\"Accuracy\"] + acc)\n",
    "t.add_row([\"Positives\"] + positives)\n",
    "t.add_row([\"Negatives\"] + negatives)\n",
    "t.add_row([\"True Positives\"] + truePositives)\n",
    "t.add_row([\"True Negatives\"] + trueNegatives)\n",
    "t.add_row([\"False Positives\"] + falsePositives)\n",
    "t.add_row([\"False Negatives\"] + falseNegatives)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEGATIVE Log-likelihood\n",
    "def f2(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= (log(10)*logit)\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime2(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= (log(10)*X[i][k])\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return np.array([-x for x in dl])\n",
    "\n",
    "def train2(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f2, [0]*len(X[0]), fprime2, pgtol = 10, args = (X_train, y_train, lam))\n",
    "  return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|   Lambda = 1.0  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.5063898722025559 | 0.5093603744149766 | 0.5051305130513052 |\n",
      "|    Positives    |        2938        |        2859        |        2867        |\n",
      "|    Negatives    |       13729        |       13807        |       13798        |\n",
      "|  True Positives |        8440        |        8489        |        8418        |\n",
      "|  True Negatives |        8227        |        8177        |        8247        |\n",
      "| False Positives |        377         |        346         |        350         |\n",
      "| False Negatives |        7850        |        7831        |        7897        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "lam = 1.0\n",
    "theta = train2(lam)\n",
    "\n",
    "acc = []\n",
    "positives = []\n",
    "negatives = []\n",
    "truePositives = []\n",
    "trueNegatives = []\n",
    "falsePositives = []\n",
    "falseNegatives = []\n",
    "\n",
    "for (x, y) in zip(corpusX, corpusY):\n",
    "    _acc, _positives, _negatives, _truePositives, _trueNegatives, _falsePositives, _falseNegatives \\\n",
    "        = performance(theta, x, y)\n",
    "\n",
    "    acc.append(_acc)\n",
    "    positives.append(_positives)\n",
    "    negatives.append(_negatives)\n",
    "    truePositives.append(_truePositives)\n",
    "    trueNegatives.append(_trueNegatives)\n",
    "    falsePositives.append(_falsePositives)\n",
    "    falseNegatives.append(_falseNegatives)\n",
    "    \n",
    "\n",
    "t = PrettyTable(['Lambda = ' + str(lam), 'Train', 'Validation', 'Test'])\n",
    "t.add_row([\"Accuracy\"] + acc)\n",
    "t.add_row([\"Positives\"] + positives)\n",
    "t.add_row([\"Negatives\"] + negatives)\n",
    "t.add_row([\"True Positives\"] + truePositives)\n",
    "t.add_row([\"True Negatives\"] + trueNegatives)\n",
    "t.add_row([\"False Positives\"] + falsePositives)\n",
    "t.add_row([\"False Negatives\"] + falseNegatives)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = [0, 0.01, 0.1, 1, 100]\n",
    "labels  = [\"Training Set\", \"Validation Set\", \"Testing Set\"]\n",
    "corpusX = [X_train, X_validation, X_test]\n",
    "corpusY = [y_train, y_validation, y_test]\n",
    "\n",
    "acc = []\n",
    "positives = []\n",
    "negatives = []\n",
    "truePositives = []\n",
    "trueNegatives = []\n",
    "falsePositives = []\n",
    "falseNegatives = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    theta = train(lam)\n",
    "    for (x, y) in zip(corpusX, corpusY):\n",
    "        _acc, _positives, _negatives, _truePositives, _trueNegatives, _falsePositives, _falseNegatives \\\n",
    "            = performance(theta, x, y)\n",
    "            \n",
    "        acc.append(_acc)\n",
    "        positives.append(_positives)\n",
    "        negatives.append(_negatives)\n",
    "        truePositives.append(_truePositives)\n",
    "        trueNegatives.append(_trueNegatives)\n",
    "        falsePositives.append(_falsePositives)\n",
    "        falseNegatives.append(_falseNegatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|    Lambda = 0   |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7226855462890742 | 0.7141485659426378 | 0.7191119111911191 |\n",
      "|    Positives    |       12389        |       12238        |       12349        |\n",
      "|    Negatives    |        4278        |        4428        |        4316        |\n",
      "|  True Positives |       12045        |       11902        |       11984        |\n",
      "|  True Negatives |        4622        |        4764        |        4681        |\n",
      "| False Positives |        3300        |        3329        |        3308        |\n",
      "| False Negatives |        1322        |        1435        |        1373        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|  Lambda = 0.01  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7228655426891463 | 0.7142085683427337 | 0.7191119111911191 |\n",
      "|    Positives    |       12392        |       12239        |       12349        |\n",
      "|    Negatives    |        4275        |        4427        |        4316        |\n",
      "|  True Positives |       12048        |       11903        |       11984        |\n",
      "|  True Negatives |        4619        |        4763        |        4681        |\n",
      "| False Positives |        3300        |        3329        |        3308        |\n",
      "| False Negatives |        1319        |        1434        |        1373        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|   Lambda = 0.1  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7225055498890022 | 0.7141485659426378 | 0.7190519051905191 |\n",
      "|    Positives    |       12392        |       12240        |       12348        |\n",
      "|    Negatives    |        4275        |        4426        |        4317        |\n",
      "|  True Positives |       12042        |       11902        |       11983        |\n",
      "|  True Negatives |        4625        |        4764        |        4682        |\n",
      "| False Positives |        3303        |        3330        |        3308        |\n",
      "| False Negatives |        1322        |        1434        |        1374        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|    Lambda = 1   |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7212455750884982 | 0.7137885515420617 | 0.7189318931893189 |\n",
      "|    Positives    |       12463        |       12300        |       12408        |\n",
      "|    Negatives    |        4204        |        4366        |        4257        |\n",
      "|  True Positives |       12021        |       11896        |       11981        |\n",
      "|  True Negatives |        4646        |        4770        |        4684        |\n",
      "| False Positives |        3349        |        3363        |        3339        |\n",
      "| False Negatives |        1297        |        1407        |        1345        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|   Lambda = 100  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.6717465650686987 | 0.6681267250690027 | 0.6715871587158716 |\n",
      "|    Positives    |       15038        |       14981        |       14957        |\n",
      "|    Negatives    |        1629        |        1685        |        1708        |\n",
      "|  True Positives |       11196        |       11135        |       11192        |\n",
      "|  True Negatives |        5471        |        5531        |        5473        |\n",
      "| False Positives |        5049        |        5084        |        5008        |\n",
      "| False Negatives |        422         |        447         |        465         |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for lam in lambdas:\n",
    "    t = PrettyTable(['Lambda = ' + str(lam), 'Train', 'Validation', 'Test'])\n",
    "    t.add_row([\"Accuracy\"] + acc[idx:idx+3])\n",
    "    t.add_row([\"Positives\"] + positives[idx:idx+3])\n",
    "    t.add_row([\"Negatives\"] + negatives[idx:idx+3])\n",
    "    t.add_row([\"True Positives\"] + truePositives[idx:idx+3])\n",
    "    t.add_row([\"True Negatives\"] + trueNegatives[idx:idx+3])\n",
    "    t.add_row([\"False Positives\"] + falsePositives[idx:idx+3])\n",
    "    t.add_row([\"False Negatives\"] + falseNegatives[idx:idx+3])\n",
    "    print(t)\n",
    "    idx += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Network visualization ###\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = set()\n",
    "nodes = set()\n",
    "for edge in urlopen(\"http://jmcauley.ucsd.edu/cse255/data/facebook/egonet.txt\"):\n",
    "  x,y = edge.split()\n",
    "  x,y = int(x),int(y)\n",
    "  edges.add((x,y))\n",
    "  edges.add((y,x))\n",
    "  nodes.add(x)\n",
    "  nodes.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFCCAYAAADGwmVOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "for e in edges:\n",
    "  G.add_edge(e[0],e[1])\n",
    "nx.draw(G)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Connected Components:  3\n",
      "Largest Connected Component: 40\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Connected Components: \", nx.number_connected_components(G))\n",
    "print(\"Largest Connected Component:\", max([len(x) for x in nx.connected_components(G)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low IDs:   [697, 703, 708, 713, 719, 729, 745, 747, 753, 769, 772, 774, 798, 800, 803, 804, 805, 810, 811, 819]\n",
      "High IDs:  [823, 825, 828, 830, 840, 856, 861, 863, 864, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893]\n",
      "Normalized Cut Cost:  0.4224058769513316\n"
     ]
    }
   ],
   "source": [
    "largest_cc = sorted(max(nx.connected_components(G), key=len))\n",
    "\n",
    "half = round(len(largest_cc)/2)\n",
    "\n",
    "split_lo = largest_cc[:half]\n",
    "split_hi = largest_cc[half:]\n",
    "\n",
    "print(\"Low IDs:  \", split_lo)\n",
    "print(\"High IDs: \", split_hi)\n",
    "print(\"Normalized Cut Cost: \", (1/2)*nx.normalized_cut_size(G, split_lo, split_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cost:  0.4224058769513316\n",
      "current cost:  0.1624924461156248\n",
      "current cost:  0.09817045961624274\n",
      "\n",
      "Number of Iterations:  3\n",
      "Cluster 1:  [729, 804, 825, 861, 863, 864, 876, 878, 882, 884, 886, 888, 889, 893]\n",
      "Cluster 2:  [697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 805, 810, 811, 819, 823, 828, 830, 840, 856, 869, 880, 890]\n",
      "Normalized Cut Cost:  0.09817045961624274\n"
     ]
    }
   ],
   "source": [
    "largest_cc = sorted(max(nx.connected_components(G), key=len))\n",
    "\n",
    "half = round(len(largest_cc)/2)\n",
    "\n",
    "split_lo = largest_cc[:half]\n",
    "split_hi = largest_cc[half:]\n",
    "\n",
    "prevCost = 0;\n",
    "currCost = (1/2)*nx.normalized_cut_size(G, split_lo, split_hi)\n",
    "\n",
    "iter = 0\n",
    "while prevCost != currCost:\n",
    "    iter += 1\n",
    "    prevCost = currCost\n",
    "    print(\"current cost: \", currCost)\n",
    "    lo = [i for i in split_lo] # deep copy\n",
    "    hi = [i for i in split_hi] # deep copy\n",
    "    \n",
    "    # move lo to hi if cut cost is lower\n",
    "    for val in split_lo:          \n",
    "        lo.remove(val)\n",
    "        hi.append(val)\n",
    "        newCost = (1/2)*nx.normalized_cut_size(G, lo, hi)\n",
    "        \n",
    "        if(newCost > currCost): # not lower, restore\n",
    "            lo.append(val)\n",
    "            hi.remove(val)\n",
    "        else:\n",
    "            currCost = newCost # update current cost\n",
    "\n",
    "    # move hi to lo if cut cost is lower\n",
    "    for val in split_hi:\n",
    "        lo.append(val)\n",
    "        hi.remove(val)\n",
    "        newCost = (1/2)*nx.normalized_cut_size(G, lo, hi)\n",
    "        \n",
    "        if(newCost > currCost): # not lower, restore\n",
    "            lo.remove(val)\n",
    "            hi.append(val)\n",
    "        else:\n",
    "            currCost = newCost # update current cost\n",
    "           \n",
    "    split_lo = [i for i in lo] # deep copy\n",
    "    split_hi = [i for i in hi] # deep copy\n",
    "    \n",
    "print(\"\\nNumber of Iterations: \", iter)\n",
    "print(\"Cluster 1: \", sorted(split_hi))\n",
    "print(\"Cluster 2: \", sorted(split_lo))\n",
    "print(\"Normalized Cut Cost: \", (1/2)*nx.normalized_cut_size(G, split_lo, split_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def totalEdges(cluster):\n",
    "    count = 0\n",
    "    for i in range(0, len(cluster)):\n",
    "        for j in range(i, len(cluster)):\n",
    "            if(G.has_edge(cluster[i], cluster[j])):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def totalAdjEdges(cluster1, cluster2):\n",
    "    count = 0\n",
    "    for i in range(0, len(cluster1)):\n",
    "        for j in range(0, len(cluster2)):\n",
    "            if(G.has_edge(cluster1[i], cluster2[j])):\n",
    "                count += 1\n",
    "    return count\n",
    "    \n",
    "def ekk(cluster, numEdges):\n",
    "    count = 0\n",
    "    for i in range(0, len(cluster)):\n",
    "        for j in range(i, len(cluster)):\n",
    "            if(G.has_edge(cluster[i], cluster[j])):\n",
    "                count += 1\n",
    "    return count/numEdges\n",
    "\n",
    "def ak(cluster, numEdges, numAdjEdges):\n",
    "    count = numAdjEdges\n",
    "    for i in range(0, len(cluster)):\n",
    "        for j in range(i, len(cluster)):\n",
    "            if(G.has_edge(cluster[i], cluster[j])):\n",
    "                count += 1\n",
    "    return count/numEdges\n",
    "\n",
    "def findQ(clusters, numEdges, numAdjEdges):\n",
    "    Q = 0\n",
    "    for c in clusters:\n",
    "        Q += ekk(c, numEdges) - ak(c, numEdges, numAdjEdges)*ak(c, numEdges, numAdjEdges)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current modularity:  -0.4288016528925619\n",
      "Current modularity:  -0.39396694214876027\n",
      "\n",
      "Number of Iterations:  2\n",
      "Cluster 1:  [697, 703, 713, 798, 828, 830, 840, 856, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893]\n",
      "Cluster 2:  [708, 719, 729, 745, 747, 753, 769, 772, 774, 800, 803, 804, 805, 810, 811, 819, 823, 825, 861, 863, 864]\n",
      "Max Modularity Cost:  -0.6776033057851241\n"
     ]
    }
   ],
   "source": [
    "largest_cc = sorted(max(nx.connected_components(G), key=len))\n",
    "\n",
    "half = round(len(largest_cc)/2)\n",
    "\n",
    "split_lo = largest_cc[:half]\n",
    "split_hi = largest_cc[half:]\n",
    "\n",
    "numEdges = totalEdges(largest_cc)\n",
    "numAdjEdges = totalAdjEdges(split_lo, split_hi)\n",
    "\n",
    "prevQ = 0;\n",
    "currQ = findQ([split_lo, split_hi], numEdges, numAdjEdges)\n",
    "\n",
    "\n",
    "iter = 0\n",
    "while prevQ != currQ:\n",
    "    iter += 1\n",
    "    prevQ = currQ\n",
    "    numAdjEdges = totalAdjEdges(split_lo, split_hi)\n",
    "    \n",
    "    print(\"Current modularity: \", currQ)\n",
    "    lo = [i for i in split_lo] # deep copy\n",
    "    hi = [i for i in split_hi] # deep copy\n",
    "    \n",
    "    # move lo to hi if cut cost is lower\n",
    "    for val in split_lo:          \n",
    "        lo.remove(val)\n",
    "        hi.append(val)\n",
    "        newQ = findQ([lo, hi], numEdges, numAdjEdges)\n",
    "        \n",
    "        if(newQ < currQ): # lower, restore\n",
    "            lo.append(val)\n",
    "            hi.remove(val)\n",
    "        else:\n",
    "            currQ = newQ # update current cost\n",
    "\n",
    "    # move hi to lo if cut cost is lower\n",
    "    for val in split_hi:\n",
    "        lo.append(val)\n",
    "        hi.remove(val)\n",
    "        newQ = findQ([lo, hi], numEdges, numAdjEdges)\n",
    "\n",
    "        if(newQ < currQ): # lower, restore\n",
    "            lo.remove(val)\n",
    "            hi.append(val)\n",
    "        else:\n",
    "            currQ = newQ # update current cost\n",
    "           \n",
    "    split_lo = [i for i in lo] # deep copy\n",
    "    split_hi = [i for i in hi] # deep copy\n",
    "    \n",
    "print(\"\\nNumber of Iterations: \", iter)\n",
    "print(\"Cluster 1: \", sorted(split_hi))\n",
    "print(\"Cluster 2: \", sorted(split_lo))\n",
    "print(\"Max Modularity Cost: \", findQ([split_lo, split_hi], numEdges, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
