{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "  for l in urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print(\"Done\")\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], datum['review/palate'], datum['review/overall']]\n",
    "  return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV'] >= 6.5 for d in data]\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Logistic regression by gradient ascent         #\n",
    "##################################################\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= logit\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= X[i][k]\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return np.array([-x for x in dl])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shuffled:  (50000, 6)\n",
      "y shuffled:  (50000,)\n",
      "x train:  (16667, 6) x validate:  (16666, 6) x test:  (16665, 6)\n",
      "y train:  (16667,) y validate:  (16666,) y test:  (16665,)\n"
     ]
    }
   ],
   "source": [
    "# split into 1/3 train, 1/3 validation, 1/3 test\n",
    "\n",
    "Z = list(zip(X, y))\n",
    "\n",
    "random.shuffle(Z)\n",
    "\n",
    "x_shuffled, y_shuffled = zip(*Z)\n",
    "\n",
    "print(\"X shuffled: \", np.shape(x_shuffled))\n",
    "print(\"y shuffled: \", np.shape(y_shuffled))\n",
    "\n",
    "samples = len(x_shuffled)\n",
    "\n",
    "X_train = x_shuffled[0:round(samples/3)];\n",
    "y_train = y_shuffled[0:round(samples/3)];\n",
    "\n",
    "X_validation = x_shuffled[round(samples/3) + 1: 2 * round(samples/3)]\n",
    "y_validation = y_shuffled[round(samples/3) + 1: 2 * round(samples/3)]\n",
    "\n",
    "X_test = x_shuffled[2 * round(samples/3) + 1:samples]\n",
    "y_test = y_shuffled[2 * round(samples/3) + 1:samples]\n",
    "\n",
    "print(\"x train: \", np.shape(X_train), \"x validate: \", np.shape(X_validation), \"x test: \", np.shape(X_test))\n",
    "print(\"y train: \", np.shape(y_train), \"y validate: \", np.shape(y_validation), \"y test: \", np.shape(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Train                                          #\n",
    "##################################################\n",
    "\n",
    "def train(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "  return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Predict                                        #\n",
    "##################################################\n",
    "\n",
    "def performance(theta, X, y):\n",
    "  scores = [inner(theta,x) for x in X]\n",
    "  predictions = [s > 0 for s in scores]\n",
    "    \n",
    "  positives = sum(predictions)\n",
    "  negatives = len(predictions) - sum(predictions)\n",
    "    \n",
    "  correct = [(a==b) for (a,b) in zip(predictions, y)]\n",
    "    \n",
    "  truePositives = sum(correct)\n",
    "  trueNegatives = len(correct) - sum(correct)\n",
    "\n",
    "  falsePositives = sum([(a==1 and b==0) for (a,b) in zip(predictions,y)])\n",
    "  falseNegatives = sum([(a==0 and b==1) for (a,b) in zip(predictions,y)])\n",
    " \n",
    "  acc = sum(correct) * 1.0 / len(correct)\n",
    "  return acc, positives, negatives, truePositives, trueNegatives, falsePositives, falseNegatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "lam = 1.0\n",
    "theta = train(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|   Lambda = 1.0  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7212455750884982 | 0.7137885515420617 | 0.7189318931893189 |\n",
      "|    Positives    |       12463        |       12300        |       12408        |\n",
      "|    Negatives    |        4204        |        4366        |        4257        |\n",
      "|  True Positives |       12021        |       11896        |       11981        |\n",
      "|  True Negatives |        4646        |        4770        |        4684        |\n",
      "| False Positives |        3349        |        3363        |        3339        |\n",
      "| False Negatives |        1297        |        1407        |        1345        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "corpusX = [X_train, X_validation, X_test]\n",
    "corpusY = [y_train, y_validation, y_test]\n",
    "\n",
    "acc = []\n",
    "positives = []\n",
    "negatives = []\n",
    "truePositives = []\n",
    "trueNegatives = []\n",
    "falsePositives = []\n",
    "falseNegatives = []\n",
    "\n",
    "for (x, y) in zip(corpusX, corpusY):\n",
    "    _acc, _positives, _negatives, _truePositives, _trueNegatives, _falsePositives, _falseNegatives \\\n",
    "        = performance(theta, x, y)\n",
    "\n",
    "    acc.append(_acc)\n",
    "    positives.append(_positives)\n",
    "    negatives.append(_negatives)\n",
    "    truePositives.append(_truePositives)\n",
    "    trueNegatives.append(_trueNegatives)\n",
    "    falsePositives.append(_falsePositives)\n",
    "    falseNegatives.append(_falseNegatives)\n",
    "    \n",
    "\n",
    "t = PrettyTable(['Lambda = ' + str(lam), 'Train', 'Validation', 'Test'])\n",
    "t.add_row([\"Accuracy\"] + acc)\n",
    "t.add_row([\"Positives\"] + positives)\n",
    "t.add_row([\"Negatives\"] + negatives)\n",
    "t.add_row([\"True Positives\"] + truePositives)\n",
    "t.add_row([\"True Negatives\"] + trueNegatives)\n",
    "t.add_row([\"False Positives\"] + falsePositives)\n",
    "t.add_row([\"False Negatives\"] + falseNegatives)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEGATIVE Log-likelihood\n",
    "def f2(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= (log(10)*logit)\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime2(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= (log(10)*X[i][k])\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return np.array([-x for x in dl])\n",
    "\n",
    "def train2(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f2, [0]*len(X[0]), fprime2, pgtol = 10, args = (X_train, y_train, lam))\n",
    "  return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|   Lambda = 1.0  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.5063898722025559 | 0.5093603744149766 | 0.5051305130513052 |\n",
      "|    Positives    |        2938        |        2859        |        2867        |\n",
      "|    Negatives    |       13729        |       13807        |       13798        |\n",
      "|  True Positives |        8440        |        8489        |        8418        |\n",
      "|  True Negatives |        8227        |        8177        |        8247        |\n",
      "| False Positives |        377         |        346         |        350         |\n",
      "| False Negatives |        7850        |        7831        |        7897        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "lam = 1.0\n",
    "theta = train2(lam)\n",
    "\n",
    "acc = []\n",
    "positives = []\n",
    "negatives = []\n",
    "truePositives = []\n",
    "trueNegatives = []\n",
    "falsePositives = []\n",
    "falseNegatives = []\n",
    "\n",
    "for (x, y) in zip(corpusX, corpusY):\n",
    "    _acc, _positives, _negatives, _truePositives, _trueNegatives, _falsePositives, _falseNegatives \\\n",
    "        = performance(theta, x, y)\n",
    "\n",
    "    acc.append(_acc)\n",
    "    positives.append(_positives)\n",
    "    negatives.append(_negatives)\n",
    "    truePositives.append(_truePositives)\n",
    "    trueNegatives.append(_trueNegatives)\n",
    "    falsePositives.append(_falsePositives)\n",
    "    falseNegatives.append(_falseNegatives)\n",
    "    \n",
    "\n",
    "t = PrettyTable(['Lambda = ' + str(lam), 'Train', 'Validation', 'Test'])\n",
    "t.add_row([\"Accuracy\"] + acc)\n",
    "t.add_row([\"Positives\"] + positives)\n",
    "t.add_row([\"Negatives\"] + negatives)\n",
    "t.add_row([\"True Positives\"] + truePositives)\n",
    "t.add_row([\"True Negatives\"] + trueNegatives)\n",
    "t.add_row([\"False Positives\"] + falsePositives)\n",
    "t.add_row([\"False Negatives\"] + falseNegatives)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lambdas = [0, 0.01, 0.1, 1, 100]\n",
    "labels  = [\"Training Set\", \"Validation Set\", \"Testing Set\"]\n",
    "corpusX = [X_train, X_validation, X_test]\n",
    "corpusY = [y_train, y_validation, y_test]\n",
    "\n",
    "acc = []\n",
    "positives = []\n",
    "negatives = []\n",
    "truePositives = []\n",
    "trueNegatives = []\n",
    "falsePositives = []\n",
    "falseNegatives = []\n",
    "\n",
    "for lam in lambdas:\n",
    "    theta = train(lam)\n",
    "    for (x, y) in zip(corpusX, corpusY):\n",
    "        _acc, _positives, _negatives, _truePositives, _trueNegatives, _falsePositives, _falseNegatives \\\n",
    "            = performance(theta, x, y)\n",
    "            \n",
    "        acc.append(_acc)\n",
    "        positives.append(_positives)\n",
    "        negatives.append(_negatives)\n",
    "        truePositives.append(_truePositives)\n",
    "        trueNegatives.append(_trueNegatives)\n",
    "        falsePositives.append(_falsePositives)\n",
    "        falseNegatives.append(_falseNegatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|    Lambda = 0   |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7226855462890742 | 0.7141485659426378 | 0.7191119111911191 |\n",
      "|    Positives    |       12389        |       12238        |       12349        |\n",
      "|    Negatives    |        4278        |        4428        |        4316        |\n",
      "|  True Positives |       12045        |       11902        |       11984        |\n",
      "|  True Negatives |        4622        |        4764        |        4681        |\n",
      "| False Positives |        3300        |        3329        |        3308        |\n",
      "| False Negatives |        1322        |        1435        |        1373        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|  Lambda = 0.01  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7228655426891463 | 0.7142085683427337 | 0.7191119111911191 |\n",
      "|    Positives    |       12392        |       12239        |       12349        |\n",
      "|    Negatives    |        4275        |        4427        |        4316        |\n",
      "|  True Positives |       12048        |       11903        |       11984        |\n",
      "|  True Negatives |        4619        |        4763        |        4681        |\n",
      "| False Positives |        3300        |        3329        |        3308        |\n",
      "| False Negatives |        1319        |        1434        |        1373        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|   Lambda = 0.1  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7225055498890022 | 0.7141485659426378 | 0.7190519051905191 |\n",
      "|    Positives    |       12392        |       12240        |       12348        |\n",
      "|    Negatives    |        4275        |        4426        |        4317        |\n",
      "|  True Positives |       12042        |       11902        |       11983        |\n",
      "|  True Negatives |        4625        |        4764        |        4682        |\n",
      "| False Positives |        3303        |        3330        |        3308        |\n",
      "| False Negatives |        1322        |        1434        |        1374        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|    Lambda = 1   |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.7212455750884982 | 0.7137885515420617 | 0.7189318931893189 |\n",
      "|    Positives    |       12463        |       12300        |       12408        |\n",
      "|    Negatives    |        4204        |        4366        |        4257        |\n",
      "|  True Positives |       12021        |       11896        |       11981        |\n",
      "|  True Negatives |        4646        |        4770        |        4684        |\n",
      "| False Positives |        3349        |        3363        |        3339        |\n",
      "| False Negatives |        1297        |        1407        |        1345        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|   Lambda = 100  |       Train        |     Validation     |        Test        |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n",
      "|     Accuracy    | 0.6717465650686987 | 0.6681267250690027 | 0.6715871587158716 |\n",
      "|    Positives    |       15038        |       14981        |       14957        |\n",
      "|    Negatives    |        1629        |        1685        |        1708        |\n",
      "|  True Positives |       11196        |       11135        |       11192        |\n",
      "|  True Negatives |        5471        |        5531        |        5473        |\n",
      "| False Positives |        5049        |        5084        |        5008        |\n",
      "| False Negatives |        422         |        447         |        465         |\n",
      "+-----------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for lam in lambdas:\n",
    "    t = PrettyTable(['Lambda = ' + str(lam), 'Train', 'Validation', 'Test'])\n",
    "    t.add_row([\"Accuracy\"] + acc[idx:idx+3])\n",
    "    t.add_row([\"Positives\"] + positives[idx:idx+3])\n",
    "    t.add_row([\"Negatives\"] + negatives[idx:idx+3])\n",
    "    t.add_row([\"True Positives\"] + truePositives[idx:idx+3])\n",
    "    t.add_row([\"True Negatives\"] + trueNegatives[idx:idx+3])\n",
    "    t.add_row([\"False Positives\"] + falsePositives[idx:idx+3])\n",
    "    t.add_row([\"False Negatives\"] + falseNegatives[idx:idx+3])\n",
    "    print(t)\n",
    "    idx += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Network visualization ###\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edges = set()\n",
    "nodes = set()\n",
    "for edge in urlopen(\"http://jmcauley.ucsd.edu/cse255/data/facebook/egonet.txt\"):\n",
    "  x,y = edge.split()\n",
    "  x,y = int(x),int(y)\n",
    "  edges.add((x,y))\n",
    "  edges.add((y,x))\n",
    "  nodes.add(x)\n",
    "  nodes.add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAFCCAYAAADR4PWHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XtcVHX+P/DXmeEyM6iAXMpbJHlP\nUFO8dNO8VBpmkFGmrlrmppXrlrpibVqZpqK2VrrZrmH+3DbMvliGa9rmJbNETSHFS7peMgVURJEB\nhpnP748jCHKbmXOYOcy8no/HPBSY+cxnDjzmNedzPp/3RxJCCBAREZFm6dzdASIiIqodw5qIiEjj\nGNZEREQax7AmIiLSOIY1ERGRxjGsiYiINI5hTUREpHEMayIiIo1jWBMREWkcw5qIiEjjGNZEREQa\nx7AmIiLSOIY1ERGRxjGsiYiINI5hTUREpHEMayIiIo1jWBMREWkcw5qIiEjjfNzdAaJ6l5MDJCcD\nGRlAfj4QGAhERwPjxgFhYe7uHRFRnSQhhHB3J4jqRXo6MG8esHGj/HVR0Y2fGY2AEMDgwUBiIhAT\n454+EhHZgWFNnmn5cmDqVMBslkO5JpIkB3dSEjBxouv6R0TkAA6Dk+cpC+rCwrrvK4R8v6lT5a8Z\n2ESkQTyzJs+Sng7062dfUN/MZAK2bQN69FC9W0RESnA2OHmWefPkoW9nmM3y44mINIZn1uQ5cnKA\niIjKE8kcZTAAp09zljgRaQrPrMlzJCcrb0OS1GmHiEhFDGvyHBkZys6qAXkoPDNTnf4QEamEYU2e\nIz9fnXby8tRph4hIJVy6RZ4jMFCdds6eBUaNYrUzItIMTjAjz7FgATBrlvKhcL0esFpvfM1qZ0Tk\nZgxr8hxqzAavDaudEZGb8Jo1eY7wcPnsV5Lqp/2K1c6WL6+f5yAiqgbPrMmzKKlg5ghWOyMiF+KZ\nNXmWmBh5mNpkqt/nYbUzInIhnlmTZ7J31y0lWO2MiFyEZ9bkmSZOlIep4+LkUDUaK//cR4VVi6x2\nRkQuwnXW1LDl5MiBmZFRdV10jx7AunVAbi6QnIzsLVtwdPdu/F5YiCc7dQL271f23Kx2RkQuwmFw\napjS0+Vrxhs3yl9XXK5Vw7rorVu3YtasWfjxxx9ROGAA9GWPVSI2FvjqK+XtEBHVgsPg1PAsXy7P\n+E5NlUP65nXVZrP8vdRU+X7Xl1nZbDbo9Xo0b94chX5+6vQlOFiddoiIasFhcGpYyiaO2bM0q+K6\naAC2tm2h0+nQokULnAsNhdHXFz4Wi/N9MRqBqCjnH09EZCcOg1PDoWQNtcmEH+fPx9R//xsXLlxA\n8ZkzOFJUBD+bzfn+cDY4EbkIw5oajvh4eWjbiT9ZmyRha2AgHr52DV26dEH//v0x/9gxp9sTAKSu\nXYGff676w9omvTHYicgJDGtqGFSo+23R6/F8z54YExIC47FjiAkLA3btqrxphwOsvr7Q/+1vN+qE\nOzHpjYjIHgxrahhU2FHLJkmwCQHcdK1aAHC2mniJjw90O3bA5+ef7SvCws1AiMgJnGBGDUNGhuLd\ntHRCyMsfbppUpmTbD5/SUhwbMABtbTbo7OnfTZPeGNhEZA8u3aKGIT/f3T2olg5Au8JC+4K6orLA\n3rOnXvpFRJ6FYU0NQ2Cgu3tQI6fPzLkZCBHZiWFNDUN0tLxUypMIIc9Gf/11uSQqEVENOMGMGgYV\nZoNrlo+PfONMcSKqAcPaHbgO1zkK1lk3CJwpTkQ1YFi7EtfhKqOkgllDYjJVDmx+uCPyegxrVymr\nac11uIqcefVVhMydC5O7O1LfTCZg2TJg/Xp+uCMihrVLOLL5RJmbz64Iv/32G+6++2588eCD6PHp\np3V/8Gno9HrAZuOHOyJiWNc7hZtPYNs2oEcP1bvV0Fy5cgX33XcfRo4cienTp8vrk4cNA37/3d1d\n0wZ+uCPyaAzr+qZkUpQkAXFxwLp1dd/Xg69rWiwWxMbGIjIyEsuWLYMkSZ49O9xZ/HBH5LEY1vVJ\njUCpaxtGD5+0JoTAc889h/PnzyM1NRU+Ptcr5KpQK9zjOPLhjogaFBZFqU/JycrbkKSa21m+XB5i\nT02VQ+vm4DKb5e+lpsr3W75ceX9cbO7cudi3bx/+/e9/3whqQJVa4R5HCCAtjQVWiDwQN/KojlpD\nymoEitkMZGZW/b4jk9Ya6OYRa9aswYoVK7Br1y40atSo8g81Wivc7co+3E2b5u6eEJGKGNYV1Tak\n/MUX8rCrI0PKagVKXl7Vfjo6uxy4EdgxMZq/rrlt2zb8+c9/xn//+180b9686h00XCvcrWr6cEdE\nDRqHwcvUx5CyWoESHFz563nz5P44owFsHpGVlYWEhAR8+umn6Ny5c/n3rVYrLl++jNOnT+NceDis\nfn5u7KWG3fzhjogaPJ5ZA/UypHz8+HGcvngRfSQJBgVz+ASAL//1Lyw+dQpDhw7FHx5+GOEbNzq/\nvrjidU0XzRIvLi7GlStX6rxdvXoV2dnZ2LBhA26//XYkJiZW+rnZbEbjxo3RpEkThNps2FlSAqNL\nXkEDc/OHOyJq8BjWKg4p//rrr1i7di3Wrl2Ls2fPYlK/fuir0wFWq9PdkwAMliR8ceECZs+ejZxp\n0/AGoCyk7LiuKYTAtWvX7ArZmwP35u8JIRAYGIgmTZpUuZWFb5MmTRAUFISvvvoKsbGxeO6556rc\nNyAgAGfOnEF8fDz27duHzXo9HrFaoVdyLDyN0QhERbm7F0SkMi7dUrgO+urAgVjaty/Wrl2L8+fP\nIz4+Hs9EReGuTZug27QJKCmRq1ApUbYkZ8YMlA4bBp9z55S1ByC9Qwf8/e67awzbq1evwmg01hqu\n9t78/f3r7I/VakVcXBxCQkKwcuVKeS11BRaLBS+++CL+8Y9/wM/PD+Hh4Uh+4QX0mz0bkrOXBDxR\nXUv9iKhB8u6wVmEddBGA2c88g8F/+APuvfde6FessK8GuIOsOh2sOh18S0sh1X33Oh3v1Anf/fnP\nNQZso0aNKi+VqkdCCEyePBlZWVlIS0uD303XotesWYM//vGPKCoqQkBAAObOnYsJEybAYrFg4R13\nYHp2Noxe/GdcjuusiTyWdw+Dq7AO2t9oxDsdOsBy990oXLwYAa+/Dl09rP/V2WzQKz1Dr+CO7t1x\nx/jxqrVXIzuWwb377rv47rvv8P3331cK6qNHj+LRRx/FkSNH4Ofnh6lTp2LmzJmwWq14//338frr\nr0Ov1+O+4cPxwIYNQFERJG8ObaNRXqlARJ5HeLORI4WQz38V3T4BRAwgClRoyxW3a4B4PyJCLFy4\nUJw7d65+ju3u3ULExQlhMMi3in0wGuXvxcWJb995R7Ro0UKcOnWq/KFFRUUiISFBSJIk9Hq9GD58\nuPjtt9/Ejz/+KMaOHSuCgoJE27ZtxV133SV27twpnn76adG/SRPxOSDMgLBp4Bi7/GYyCbFsWf38\nLonI7bw7rGNjVXmj3Nm0qdjVrJmwuvsN286bxcdHPHTXXcJkMglJkkSTJk1E3759xdKlS8XFixeV\nH9dly+TwkKRa+2GTJHENEKcSE8sf+sEHHwhfX18hSZLo2rWr+PHHH8Xf//530bVrV3HHHXeI+fPn\niyeeeEI0atSovP9hYWHCZDKJF154QVh+/12IuDhh0+vdfpxdcpMkBjWRF/Dua9ajRgFr1ihu5tum\nTXHf5cvwU3GYut7cdF3z+PHjWL16NTZs2IBDhw6hqKgIQUFB6N69O5544gmMGDECjRs3tr99J7cD\nPfOnP2HwP/6Bwbm56Onvj5h27ZBdVIS0M2fwv3790OSOO7Br1y4cOHAAVqsVHTp0wKhRo3DhwgWk\npKRg9erV6N+/Pw4fPoxZs2ah1YYNeLukBL5WK3Se8Cfu4wOUlt74uqzu+5Ah8tC3xovcEJEy3h3W\nKmwGUazX43irVrjjzBn4K1ii5So2nQ5n33wTwX/6U9USnpCvEycnJ2Pjxo3IyspCSUkJQkJC0LNn\nTyQkJCAhIQFGYw0LxxRsB1p6/Sbp9ZWOo1mSACHwjU6HzyIjsSE7Gzt27EBQUBBGjBgBo9GIl156\nCfv378enn36KEydOwNfXF40aNUJcq1YYn5uLLr//DgE0iN9Ptfz9gRkzgBMn5IInwcHy8qyxYznr\nm8hLeHdYqzAb3KLX4z++vhjaQDaVsAEoliRM1+uR0rQpWrdujcjISERGRlb6f8uWLaHX65GVlYWV\nK1di06ZNOHLkCCwWC8LCwtC7d2+MGDEC8fHxNyaFKVgGJ4BaZ7kLSYJZCGwcOBAf+/vjm2++gb+/\nP2w2G3x8fHDt2jU0bdoUJpMJBQUFuHr1KkpKSgAALf398bpOh3Fmc8ObUckZ3kQEbw9rQFHAWAEU\nDBiAxgYDdF9/rX7f6lGJry+2PPww0rt3R2FhIa5cuYILFy7g3LlzOHXqFHJyctCqVatKQd66dWvY\nbDZ899132LFjB3799VeUlpbi1ltvxeDu3bFi0yboLZZ67fc1AFMBfKTXw3r9TFmv1yMkJATNmzfH\nbbfdhtatW+P2229HREQErl27hvT0dDy+Zg3uuXix4RVQ4R7VRASGtaKhW4ufHwYbjVgSEYGojAz1\n+1bPrgEYoNdj//X11EIIWCwW6HQ6BAQEwGQywWAwwNfXF4BcuMRsNiMvLw96vR4tW7ZE06ZNUVBQ\ngMdPnMC0a9dgckG/CwH0kyScDA1FYGAgzGYzrl27hsLCQlgsFuj1ciTbbDYIIdBMr8fx0lIYXNA3\nVZlMQFJSg9kljYjqD8MacHpSFJKScCY2Ft8MHIiRx44pqgHuDjZJwvGoKCzr3x9nz57F+fPnkZub\ni0uXLiE/Px8WiwW+vr6Vws9qtaK0tBRCiPIqY0IIfAJgtIv6bQXwjcmExLZtodfrUVJSgoKCAuTm\n5kKSJLRr1w6dO3dGt27d0L59e3T46itErFwJXXGxi3qokCTJE8gY1ER0HcO6TFlg11V5rJo3UpGd\nDWuLFvBpgBOYrH5+2PLPf+Kyr2/52WnZv/n5+Th//jzOnz+PixcvIj8/HwUFBTCbzbBYLOXD0ADw\nJYChLuy3GUAEgIs6HXTXb4B89l/WL51OB71ej2SrFU9rcaa+Tle5FC1neBNRDRjWFe3ZI28fmZYm\nh3LFmtN1vZE2bw5x7pwqpUBdySxJ+FvTpviwcWNYLBYUFxejpKQEJSUl5YFcdmYthIDNZoNer4de\nr4ckSeXhuEanwwgXflgp8fHB7kcewb7+/XHhwgXk5uZW+ffSpUsICAjAOosFA5y4zFGvjEZg/Hjg\n8mXO8CaiOjGsq5ObK5fIzMy07400Jwdo1UretKMBWg1gvJ8fGjVqBKPRCH9/f+h0OgghUFRUhPz8\nfBQXFyMsLAzNmzfHrbfeivDwcNxyyy0ICwtDUFAQOqeloWtqKnwrrgWuZ2mhoUh+4AFERkbitttu\nQ2hoKMLCwhAaGorQ0FCEhITIM9VVWk+vGl6LJiIHNbiVLC4RFlbr9pFVJCfLQ5oNVIheDyEELl++\njKKiIhiNRphMJgQEBCA4OBht27ZFQEBApTPqixcvIjc3FxaLBRcvXsSFQ4dwsLQUvi7sd0BJCfbs\n2YMNGzbAZDKhXbt2aNOmDdq2bYu2bduW/z8wOlpe+qSF5XUGA4OaiBzGsFZDRoY2gsBJrbt1w4oX\nXoAkSbhy5Qry8vJw+fLlSv/+9ttv5V8XFBSgcePG8Pf3Lz/rDggIwO6mTXHvpUsuWx5lbNYMUe3b\no0mTJjh16hT27t2L//3vf9ixYwcAoKioCHl5eWjl749fiotR90adLtCnD4OaiBzGsFZDfr67e+A0\nsyTh29xcbFy7FsHBwQgKCkJwcDBatGiBzp07l39d9q/RaERqaireeOMNXLhwAR07dsR7772H33//\nHXOefx5pfn7Qu+BygM1gQM9nn8X6CiMgV69exZkzZ3D69GmcOnWq/N9jx47h2wMH8FBxsfvXWe/a\nJV9m4XVpInKEy6qQezKVdu9yy81gECInp86XmJeXJ958803RpEkT4efnJ3r37i1++uknUVhYKP74\nxz+KNm3aiH379t3YxKOe+20GxOAePURKSoooLi6u+3e0e7ewOdkvGyBKdDp1+m40CrFggQp/dETk\nTRruhVYtiY6Wr0U2NJIkz26v5Szv9OnTeOmll9CiRQvMnTsXPXr0wI8//ohdu3YhKCgIffr0weXL\nl7F3715069ZNHuJNSpInUUn1MzdeSBIOR0Zi16+/4plnnkFISAgmTZqEEydO1PygmBhIZf1y5LkA\nlEoSNkoSjivrtsxslicuEhE5wt2fFjxCdnbVPZsbws1kEiI9vdqXtH//fpGQkCCMRqMwGo1i6NCh\nIiMjo/zn//rXv0RoaKhYvny5sNlsVRtITxciPl4+LkZj1bNLPz8hnD1bvd7v0tJS8e2334rhw4cL\ng8EgfHx8RIcOHcTKlStFSUlJ9b+rZcuEzcfH4T2vr0mSKGrbVp3jHhurxl8dEXkRhrVa4uLq3L9Z\nU7dq9kC22Wxi06ZNol+/fqJx48bCZDKJhIQEkZWVVX6fwsJCMWHCBNG2bVvx888/131ccnLkYd/R\no+WQGj1a/jonx7kh8xr2bi4uLhaff/65uPvuu4Verxd+fn4iNjZWHDhwoPIdd+8WFh8f546ZSntk\nfx8ZKebPny/Wr18vjhw5IiwWi1N/ckTkPbjOWi0KaoxXYTDU3+zyaiqwWSwWfPbZZ3jnnXeQnZ0N\ns9mMxx9/HH/961/Rpk2b8oceOXIECQkJ6NSpEz788EM0adJEeX/srBxnA1AkSTj87LPotmJFeanT\n6hQWFiJ5wQJcee89tLx0CeF+fgi54w50euop+O3aBfznP05NNBOSBAEo2h/b6ueHPUOHIiUiAocP\nH8bhw4dx9uxZREZGokOHDuW3jh07ov31me4NQk6OvIQxI0OecBkYKF8eGjeOk+mIVMCwVpMzNcaN\nRuDhh4FGjSoXYNm6Fdi4sdYAq5UdpSyvXLmCjz76CEuWLIFer8elS5cwYsQIJCYmonXr1pWaW7Nm\nDaZMmYK3334bzz33XK1h6TA7K8f9/PDDGLlkCSIjI/H+++/j9ttvr9pWerrc1saN8tcVPvQUAjCi\n9q046yIUPh4GA3D6dKUAM5vN+PXXX5GVlVUe4IcPH8aRI0cQFBRUKcTLbi1btlT3d+CsWo53+e9u\n8GD5by4mxj19JPIADGu1KagxXomSM/U6SlmePXsWS5cuxUcffYSwsDCcO3cOY8aMwfTp09GqVatK\nTRUWFmLy5MnYsWMHUlJS0KVLF8f7Yy87KseVlJQgKSkJixcvxl/+8hdMmTKlfFcwu4+9AmWtOhWT\nDu5NbbPZ8Ntvv5WHd8UwLygokDcpuSnE27ZtC39/F60oV+tvnYjqxLCuD0pqjFekYDew6t4Uf/nl\nFyQlJSE1NRURERE4efIknn32WUydOhXNmzevcv/Dhw/jiSeeQFRUFD788EM0btzY/n7Us+PHj2PS\npEk4d+4cPvzwQ/TZv9/xY+VqKu5NffnyZRw5cqTSmXhWVhZOnjyJli1bomPHjlWCPCQkRIUXcZ3K\nf5tEVDuGdX1ytMZ4da6/KdrM5tqvldZw9iKEwNatW7Fw4ULs2bMHEREROHbsGCZMmIBXXnkFt9xy\nS7XNrV69Gi+//DLmzp2L8ePHa2PI9SZCCHz22WdY9eKLSM3Ph78L65I7zEVBVVJSghMnTlQK8bIg\n9/Pzq3RNvOz/ERER5Zu12EXJqI+KH1iIvAnDuiHYswffDhyIfmaz/KZqx5l6aWkpPv/8cyQlJeHS\npUto1qwZsrKyMGnSJEyZMgWhoaHVPlVhYSFeeukl7Ny5EykpKYiOjnbFK1SkJDYWPl9/DU0WDdDI\nELAQAufPn68S4ocPH0Zubi7atGlTJcjbtWuHgICAqo3FxwOpqc5danDwUgARyRjWWlPNrNqcW29F\n/9WrceDAAehXr671TL2goAArV67EkiVLEBISgkaNGiEzMxOTJ0/G5MmTERwcXONTHzp0CAkJCejW\nrRuWL1+ORo0aueY1K5GTA0REaLM2u04HPPaY5vemvnbtWpUh9cOHD+PYsWMIDw+vNJQedcstuOfp\npyEVFzv/hNVMsiOi2jGstaKWWbUWHx9ACPg++miNs2rPnz+P9957DytWrEC3bt0ghMCBAwcwZcoU\nvPDCCwgMDKz16T/55BO88sormD9/PsaNG6fJYe9qLVgAzJqlzbD29wfOnGmwoWS1WnHq1KlKk9u6\nbdmCcSdPwqikYaMReOMNx3a2I/J2rl3WTdUqKw5SV1EVSapSFCQrK0uMHz9eBAcHiyeeeEIMGjRI\n3HLLLWLBggXi6tWrdT51QUGBGDt2rOjQoUOlCmUNhpbrsntiHXC1jvfo0e5+JUQNiiYv83mVirNq\n6xrkEAIoLISYOhXHXnkFjz76KPr27QtJkhATE4OdO3diyJAhOHHiBKZNm1bnMPahQ4fQs2dPWK1W\npKenIyoqSsUX5iJa3vHME+uAq3W88/LUaYfIS3CLzPpWW2WnkyedWm4kFRaixZIlGDByJMzR0di4\ncSNmzJiB9evXw2DnhiLJycmYNm0aFixYgLFjxzacYe+b1TG873aeFkpqHe9a5k4QUVUM6/pSW2Wn\nL76Qr7MGBzu9LtggBNqtW4cn3n0XY8aMsbsQxrVr1zBp0iSkp6dj69atuPPOO516fs2IjpZnFmvx\nmjXgeaGkxvE2GuWJkURkNw6D14fly+V1qKmp8pvazW9sZrP8vXPnnH4KHYCHhcCEuDi7g/qXX35B\nTEwMJElCenp6ww9qQJ4Jr4abRyR8VPgc64mhpMbxFkK93xuRl2BYq82Ra9AKSZIkD7HXQQiBlStX\n4oEHHsD06dORnJxc/frZhig8XK497ewwviQBjzwCvPkmMHo0EBsr/ztzpjybWwlPDCU1jncde6gT\nUVUcBldTerprS17aMYGpoKAAkyZNwt69ez1j2Ls6iYnApk3O11GfPbv6ddCZmcqKf3hqKCk93omJ\n6veJyMPxzFpN8+ZVri7mCtnZNf4oMzMTMTEx8PHxwe7duz0zqAF53XlSklzK0hFlJUBrKliSmCiH\nizM8OZTq63gTUc3cvXbMY2RnC2EwuH4tr04nRFycELt3l3fFZrOJf/zjHyI0NFQkJye78aC4mIL1\n6nW26cjvxN62G7r6ON5EVC2GtVrmz3dPWN/0Znj16lUxcuRIceedd4qDBw+6+6i4Xnq6EPHx8u/C\naKxapMRgkH+enm5/mwylmtXH8SaiKlhuVC2jRgFr1ri1CzaDAW82aYLfhg7F0qVLYXJ0mNKTqLHj\nWUVqbXvqqdQ+3kRUCcNaLUOHAhs2uLsXKPXzg8/Ond4ZGK7AUCIiN2BYq0UDZ9YAuAUhEZEH4mxw\ntURHVy2s4Q5CyEO1ubnu7gkREamEYa0WFYpfqDbEYWexFCIiahgY1moJD8fVe++F1dnHSxKk5s0B\nnQq/Ek/c7YmIyIsxrFUghMCKFSsQn54Om6+vc40YjcD69cCAAep0ytN2eyIi8mIsN1pRbdtZ1jDT\nNycnB+PHj8fZs2fx2scf44PERPzx6FEYHZm3d72yk+jeHfn+/ghS47V42m5PRERejGfWgFzTOz4e\niIiQt65cs0ZehrVmjVw3+rbb5J+np1d62Ndff42uXbsiIiICUVFReP755+H70kvwXbpUDuA6NjsQ\nkgSbwYDvHnkEw7/9FrfccguWff89ivV6Za/HE3d7IiLyYly6VbZLltlc+4YNkiSHYFISCseMwdSp\nU/H111+jf//++PLLLzF+/HjMnDkTgYGB8v1rKaJh8fGBsNmwSa9HcrNmaPrgg+jbty/uu+8+RBiN\n8ocGJfsFGwzA6dNc90tE5CG8O6wrbmdpJ6vBgLeaNME3d9yBkydPom/fvpg3bx5uv/32KvctLi7G\n/s2bUfDBBxAHDsCSmwtLo0Yo7dgR+meeQc9HHkGzZs2qPkl8vLLdnrjOmojIo3hvWKenA/36ObXN\nX6Ek4aWoKExYsQK9evW68f3CQvz000/Ytm0btm/fjt27d6N9+/a4//770bdvX9x7770IDQ2t177B\nZAK2bWMFMyIiD+K9Ya3g7NUmSZAeewwFq1Zh586d2L59O7Zv3479+/cjKioKffv2xf3334977rnn\nxrC4o5w46y/fgnDiROeek4iINMk7wzonR/F14WJJQnujERE9epSHc58+fRAQEKBeP524ns6gJiLy\nPN65dEuF6l4+vr449tpr8E1MVN6fmkycCMTEcLcnIiIv551hnZGhbLY1AH1JCfRZWSp1qBY9esiT\nxbjbExGR1/LOsM7PV6cdV1YJCwsDpk1z3fMREZFmeGdRFGcnfd2MVcKIiMgFvDOs1djOklXCiIjI\nRTgb3FmsEkZERC7inWfW4eHA4MF11u6ukSTJM7AZ1ERE5ALeeWYNsEoYERE1GN55Zg3I65eTkuTg\ndURZlTAGNRERuYh3Lt0qU1bti1XCiIhIw7x3GLyiWrazZJUwIiJyN4Z1RawSRkREGsSwJiIi0jjv\nnWBGRETUQDCsiYiINI5hTUREpHEMayIiIo1jWBMREWkcw5qIiEjjGNZEREQax7AmIiLSOIY1ERGR\nxjGsiYiINI5hTUREpHEMayIiIo1jWBMREWkcw5qIiEjjGNZEREQax7AmIiLSOIY1ERGRxjGsiYiI\nNI5hTUREpHEMayIiIo1jWBMREWkcw5qIiEjjGNZEREQax7AmIiLSOIY1ERGRxjGsiYiINI5hTURE\npHEMayIiIo1jWBMREWkcw5qIiEjjGNZEREQax7AmIiLSOIY1ERGRxjGsiYiINI5hTUREpHE+7u4A\nERGRw3JygORkICMDyM8HAgOB6Ghg3DggLMzdvVOdJIQQ7u4EERGRXdLTgXnzgI0b5a+Lim78zGgE\nhAAGDwYSE4GYmNrbakCBz7AmIqKGYflyYOpUwGyWQ7kmkiQHd1ISMHFi1Z+rGfguwrAmIiLtKwvq\nwkL7H2MyVQ1stQLfxRjWRESkbenpQL9+jgV1GZMJ2LYN6NFDvcB3A4Y1ERFpW3w8kJpa+5lwTSQJ\niIsDZsxQJ/DdhGFNRETalZOD6VCnAAAZ7UlEQVQDRERUvq7sKIMB6N9fvkatJPDXrXO+DwpxnTUR\nEWlXcrI67XzzjXNBDciPS0sDcnPV6YsTGNZERKRdGRnKzqoBoKgIigeRJUm9Dw5OYFgTEZF25eer\n0oxktSprwGwGMjNV6YszGNZERKRdgYHu7sENeXlue2qGNRERaVd0tDxBTAm9Xp2+BAer044TGNZE\nRKRdY8cqvt5stdmg8Kq3XCAlKkppK05jWBMRkSaVlpbi/ZQUpAkBm5NtWAEcatUK/j4K960SAhg7\nVlkbCjCsiYhIc7Zs2YKuXbvi//7v/9Bh1SroTCan2rH5+uLvFy/CpmSCmSQBQ4a4dXMPbpFJRESa\ncfz4cbzyyivIzMzEokWLMGzYMEiSJE/ucqJUqG9SEhauXw9p0ybnO6XTyZt6uBHPrImIyO2uXr2K\nGTNmoFevXujduzcOHjyIxx57TA5qQK7NnZQEmEwQZd+riSTdqOn9+OMwbdumLOwkSa6i5kYMayIi\nchubzYbk5GS0b98e58+fR0ZGBmbMmAHDTTPACwoK8ElAACZ26oQNPj6w6PWw+vtXbsxolGeOx8XJ\ntbwnTlSnkImvr1sLogAcBiciIjfZtWsXJk+eDL1ej9TUVPTs2bPSz202G7Zt24ZVq1Zh/fr1uOee\nezBm+nQMGjoUvlevygGamSkPkQcHy7O1x46tfG1ZjQpobi6IAjCsiYjIUTk5clBmZMgVxgID5fXQ\n48bZNQnr7NmzmDFjBr777ju88847ePrpp6HT3RjoPXbsGD755BOsXr0agYGBGDNmDN555x3ceuut\nNxoxGIBp0+ruq0oV0NxZEAVgWBMRkb3S04F58+Tdq4DKZ6xffAHMmgUMHixPxoqJqfJws9mMRYsW\nYcmSJZg4cSIOHz6MRo0aAQAuX76Mzz77DKtWrcLx48cxcuRIrF+/Hl26dHGoixcvXsTPP/9cfntq\n+3Y86vQLrsCNBVEAhjUREdlj+XJ5NrbZXP3uVWaz/G9qKrBpkzy5a+JEAIAQAl988QWmTp2K7t27\nY8+ePWjdujVKS0uRlpaGVatWYdOmTRg0aBBmzpyJhx56CL6+vrV2RwiB06dPVwrm/fv34/Lly+ja\ntSu6deuGBx98EN3DwyE+/BCSkqFwNxdEAbifNRER1aUsqB1cNoWkJBy4+25MmTIFFy9exLvvvov+\n/fsjMzMTq1atwpo1axAREYExY8bgySefRNOmTattqrS0FEeOHKkSzP7+/ujWrVv5rWvXroiMjKw0\npK7aftinT7t1nTXDmoiIapaeDvTr51hQX1fs44OhjRsj7u23MWzYMKSkpGDVqlW4cOECRo8ejTFj\nxqB9+/aVHlNYWIjMzMxKwXzw4EE0b968SjBXuoZdm/h4+YzfmbiTJHl2+bp1jj9WRQxrIiKqmYKg\nswE43b07Jjdvju3bt+PRRx/FmDFj0K9fP+j1ely8eBH79++vFMz/+9//0KFDh0rBHB0djSZNmjj/\nGhR84IDJJC8D69HD+edXAcOaiIiqp8IQcrFOh3WLF6ProEE4duxYpWC+fPkyunTpUimYO3XqBD8/\nPxVfxHUKhvLLrr27E8OaiIiqt2CBPMNbQVgX6XSY6++PFU2aVArlbt26Vb2+XN/qmiRXRpLkSWUa\nCWqAYU1ERDUZNQpYs0ZxM+bhw2Fcu1aFDqlgzx55+VlamhzKZbPYATmghZA37UhMdPvQd0VcukVE\nRNVTqaCIUWkFMTX16CFPFsvNta8CmkYwrImIqHqBgeq04+aCItUKC7OvAppGcCMPIiKqXnS0vMZY\nCQ0UFPEEvGZNRETVy8mBtWVL6C0W59vQQEERT8AzayIiqmL37t0YMGIEvvXzg62u/aNrIknyZC0G\ntWIMayIiKnfo0CHEx8cjPj4eTz75JPpv3gyd0ehcY0ajPKuaFGNYExERTp06hXHjxqFfv37o06cP\njh07hgkTJsCnTx95vbHJ5FiDZQVFNLT8qSFjWBMRebGcnBxMmTIFd911F1q0aIGjR49i2rRpMFY8\nm5448UZg1zUkLkmaqvzlKRjWRERe6MqVK5g1axY6duwIm82GQ4cOYc6cOQgKCqr+ARMnyjWy4+Lk\nSWM3D40bjfL34+Lk+zGoVcXZ4EREXqSoqAjLli3D/Pnz8fDDD2P27Nlo3bq1Y400sIIinoBhTUTk\nBUpLS7Fq1Sq88cYbuOuuuzBnzhx07tzZ3d0iO7GCGRGRBxNCYN26dXjttdfQrFkzpKSkoHfv3u7u\nFjmIYU1EpLacHHmYOCNDrq8dGChXAxs3zmXDxEIIbN68GTNnzoQQAkuXLsWgQYMgObtmmtyKw+BE\nRGpJT5d3dNq4Uf664gYWZTs6DR4srz2Oiam3bvz0009ITEzE2bNnMWfOHDz++OOu3YqSVMewJiJS\ngwb2Sj548CBee+017NmzB7NmzcLYsWPh48MBVE/Aj1pEREqVBXVhYe1BDcg/LyyU7798uSpPf/Lk\nSYwZMwYPPPAA7r33Xhw9ehTjx49nUHsQhjURkRLp6TeC2hFlgb1nj9NPnZ2djcmTJ6N79+6IiIjA\nsWPH8Morr1QuaEIegR+7iIiUmDdPHvp2htksP37dOocelp+fj6SkJCxbtgyjRo1CVlYWwsPDnetD\nbTQwUY5kvGZNROSsnBwgIqLyRDJHObCFpNlsxgcffIAFCxZgyJAhmD17Nm6//Xbnn7smGpkoRzdw\nGJyIyFnJycrbkKQ62yktLcVHH32Etm3b4ocffsDWrVuRnJxcP0G9fDnQrx+QmiqH9M0fRMxm+Xup\nqfL9VLruTrXjMDgRkbMyMpSdVQNy+GVmVvsjm82Gzz//HK+99hpatmyJdevWoVevXsqerzYVJ8rV\npeJEOYC1wOsZw5qIyFn5+eq0k5dX6UshBDZt2oSZM2dCp9Phgw8+wMCBA+u3oInSiXIxMdwOsx4x\nrImInBUYqE47wcHl/921axcSExNx/vz58oImLqk65oaJcmQ/XrMmInJWdLQ8QUwBi48PzoaEIDMz\nE8OGDcOTTz6J0aNH45dffsHw4cNdE9Q5OfJkMmfnGwsBpKXJu3FRvWBYExE5a+xY5W0IgXtWrEC3\nbt1QWFiIdevW4ZlnnnFtQRMXTZQj5zGsiYicFR4uL2Fy8uzXBuA/Oh3GTJ2KzZs3o2vXrhg+fDg6\nduyI119/Hb/88gtcsrq2nifKkXJcZ01EpER6uryEydGJWQBKfHxwLS0NwYMGlX9PCIHdu3dj7dq1\nSElJQaNGjZCQkICEhAR06tRJxY5XMHQosGGD8nZiY4GvvlLeDlXBM2siIiViYuRNOUwmhx5mMxrh\nt3RppaAGAEmS0KtXLyQlJeHkyZNYuXIlrly5goceegidO3fGm2++iaysLDVfQb1MlCN1MayJiJSa\nOPFGYNcxJC4kCTCZoFu0qM61yTqdDr1798bixYtx6tQprFixApcuXcLAgQMRFRWFt956C0eOHFHe\nfxUmysFoBKKilPeFqsVhcCIitezZIy9hSkuDkCRIFZZCWf39oZckYMgQuUyngjXJNpsNP/zwA1JS\nUvD5558jLCwMCQkJeOKJJ9CuXTvHG1ShbGqJTocjmzcjqn9/p9ugmjGsiYhUJITAfz/7DBkvv4x2\nxcXo2a4dwtq2lc86x45VfQMMm82GnTt3lgf3rbfeWh7cbdq0sb+h+Hi5hKgTkSAkCUc7dUL/vDx0\n7twZ06ZNw4ABA1yz7MxLMKyJiFTaXWrnzp1ITExEbm4u3n77bcTFxbk0sKxWK77//nukpKRg3bp1\naNGiRXlwR0ZG1v5gBRPlhF4PadIkFE+fjn9t3oyFCxfCYDBg+vTpGD58OPfVVgHDmoi8l0q7S2Vk\nZODVV1/FgQMH8MYbb2D06NFuDyir1Yrt27eXB/dtt91WHtytW7eu/kGO1Aa/ic3fHzpJAgYPhu0v\nf0Fabi4WLFiAM2fO4OWXX8YzzzyDgIAAxxrlFp03CCIib7RsmRAmkxCSJIQcy9XfJEm+37JlVZo4\nfvy4GDlypAgPDxdLliwRZrPZDS+kbhaLRWzZskVMmDBBhIWFiZiYGLFw4UJx8uTJqne297jYebx2\n7dol4uPjRVhYmPjrX/8qsrOz6+7w7t1CxMUJYTDIt4rtG43y9+Li5Pt5CYY1EXmfskByJIQqBNDv\nv/8uJk2aJJo2bSpmz54trly54uYXZD+LxSI2b94snnvuOREaGip69eolFi1aJE6dOnXjTunpQsTH\ny6FoNDoX2jd9wDly5IiYMGGCCAoKEs8//7w4duxY9R1U4UOUJ2JYE5F32b3b8aC+frMZjeK9MWNE\ncHCw+POf/yxycnLc/WoUKSkpEZs2bRLPPvusCAkJEb179xaLFy8Wp0+flu+QkyPESy8Jodc7H9jp\n6ZWe8/z58+LVV18VISEhYvjw4WJ3xbNjhR+iPBmvWRORd1Ew69kKYH9EBMJ37ECrVq3U75sbWSwW\nfPvtt0hJScH69evRoUMHJCQkYMJ//gPjpk3ObfIhSUBcXLW7cRUUFOCf//wnFi9ejMjISMwZNgx3\nv/oqJCeul8NkArZtq3s5XAO+Bs6wJiLvocJ6YhgMwOnTmn9zV6KkpARbtmzBxlWrsDAlBUrKpQiD\nAdLNx6tCaNry8nDq8mWU7t2LyOJi6J15klo+FABQbSKhO7GCGRF5D+4uZRc/Pz8MGTIE73XvDn+l\nW4BaLLB9/LH8RXq6PLIREQHMmgWsWQNdWhpa//AD2jgb1EDtW3QuXy4vSUtNlUP65g9qZrP8vdRU\n+X7Llzvbi3rFsCYi78HdpRyTkQFJ4fHys1rxn4UL8cMf/oDSe++FrYbQVLwavboPURWXotU1iCyE\nfL+pUzUZ2AxrIvIe+fnqtJOXp047WqfS8Wp14QK6rF4Nn5IS6OrryuvNH6LS051bM14W2Hv2qNs/\nhRjWROQ9uLuUY1Q6XnfqdHCwHIpTRMUPUfPmyQHuDLNZfryGMKyJyGuI6GhY/fyUNeJNu0upsBuX\nFYBks6nTnzqs3bIFsbGxWDhtGqwbNjg3gx2o/Rq4mzCsicjjZWdnY9GiRei7ciUsFouyxoSQN+Tw\nBiq8Th1UuB5th0IAByUJhw4dgu3jj1Gi9PessYmEDGsiahhycoAFC4BRo4ChQ+V/Fyyo8ezHYrEg\nNTUVw4YNQ4cOHZCRkYHugwfjv35+cPo8r2yLSw9etlVJeLi8pMnJzUhs12+u4OfjA78JE9C/f3/c\n6+cHo9IGNTaRkOusiUjbHFwjm5mZiY8//hhr1qxBu3btMG7cOPj6+uKtt95Cq1atcJ/BgOkbN8Lk\nzFufvcU3PImC3big1wNWq+pdupkNQMYdd+D7e+7BgN270f7IEXUmssXGAl99pbwdFfDMmoi0y841\nsiI1FZZ778W8iAgMHjwYRqMR33//Pd5++22sWLECixcvxsyZM3H16lXs1elgnT8fFl9fx/piMgFJ\nSd4V1IBcJCQpSX79DigEcKlZs/rp080MBnQdPhwvfv45OqoV1ICmJhJyk1Eiqj9Kyjs6sF2jJAR8\nS0owPTsbf1m0CFn9+uHll19GZmYm5syZAyEEXn75Zbz++ut48cUX8eWXX+IHgwGzLBYYJQlSbW/u\nkiSfwSclARMnOvb6PUXZ6546Vf6AZMfxyp86FbsWLkR8PXdNANhvsaDL3/4GvdI19BVpbSKhG+uS\nE5GnUrrFoYLNNor0ejEwKEgsXrxYXLhwQfzhD38Q7du3Fz///LMQQoh9+/aJ0NBQMWjQILFoxIia\nd5cq62d8fJXNKLxWbbtxVXO8LvzlL6LQ2a02HdlgpT7aNRjkjUw0gtesiUhdZWfEdp6BVXvGqmCz\nDRuA0thY/PLGG3jqqadw3333YenSpQgICMC5c+fQq1cvJCQk4Msvv8T+/fthMpnkSWrJyfKEorw8\nefgzKkqeDe0tk8kcYe/xysmBuO02SMXF9dodAZVnnNdVa9wNGNZEpB4Hhq7LlV0LLgtsFTbbKPXx\nQVRgIGa9/z6eeuopAEBhYSH69euHAQMGYNWqVVi7di3uuecep5+D7BQfD5GaWvulBq3R4ERChjUR\nqUPJrOEKb46XZ85EwMKF8C0tdborRZKEa9OnI+SddwAANpsNTz31FPz8/FBaWormzZtj8eLFTrdP\nDlDyd+EON3941AjOBicidSgo7yjMZpyZNAkPPvggvlm0SFFQA4BBCIT8/nv517Nnz8Zvv/2G2NhY\n7Nu3D3PmzFHUPjnAydnkLidJmg1qgLPBiUgNOTnyOmgnB+okIRC+dy+eX7YMQwBg82blfbpeJ3rN\nmjVYvXo10tLSMGDAAKxdu1a+Tk2uU2E2uTCbtTckrtMBjz0mr9XX0NB3RQxrIlJOpbKMRxITUXzl\nCkao0VhwMH744QdMmTIF3333Hd5880089dRTvE7tLhMnAjExkObNg0hLg7W4GD5aCe3HHwdSUtzd\ni1oxrIlIORX2ifa32TDpvvvQuE8f4I03lLWn0yFfp8Pw4cOxatUqHD16FHv37sX+/fsV9ZEU6tED\nWLcO0jffQP/II4DCyx2qMBrloXqNY1gTkXIq7XscuHcv0KWL87slXSdsNvh/8gn+3/DhiO7ZE9HR\n0Rz+1pK//x2SC8qQ2qWBbMzC2eBEpNyoUcCaNeq0ZTQCxcWACtsqCpMJ/+zYEYfuv5+zv7VChaV5\nqtHgeuqacDY4ESmnwr7H5cxmVYIaAKTCQozctw9z4+u76CXZTUPbTsJolCeVNQAMayJSTsPDiAYA\nhiVL3N0NKqPC/IZKTCbgqaccXxrWwDZmYVgTkXIK9z2ui5JrdZIQQFpajftek4upNL8BwI3A/fTT\nG2u56/ob1Ph66powrIlIHYmJ8rCiFkmStoZfvVlgoDrttGolV70rC9yJE+Wv4+LkSzI3/y0ajfL3\n4+IqP66B4GxwIlJHWaUqR2uD20Hx+brZLG86Qe4XHS1P6FIyFG4wAC+9VHUI+/rSME/cmIWzwYlI\nXdc389BcparYWOCrr9zdC1JjNrjBAJw+3WCD1xkcBicidV0fjvw1KgolOp12hsaDg93dAwKUz2+Q\nJGDIEK8KaoBhTUT1QHTvjseFQPq6dXI1shYt3Nsho1EeBiVtUDK/oQEtt1ITw5qIVLd3715cu3YN\nfR59FJg2Td4i0Z0aSJUqr+HsTlwNbLmVmhjWRKS6lStXYty4cdDprr/FqFk0xVFeOmyqeRMnevxy\nKzVxghkROS8nR551m5Ehr58NDISlY0fcmZSEbzMy0KpVqxv3c1eJSZNJXqrjhWdjDcKePfJe6Glp\ncihX3BPdaJRHRYYM0fT2la7AsCYix6Wny2+wGzfKX1cI4VI/P9hKS+E3bJj8Blu2o1F8PJCaqniT\nDofNnw9Mn+7a5yTHeeByKzUxrInIMdeXZsFsrj14JUk+MyobukxPl69dq7wGu1a+vsDbb8vXzYka\nMF6zJiL7lQV1YWHdZ8hCyPebOlV+XEwMLPPmweLn55q+AoDFwmIo5BEY1kRkn/R056qTFRbC9sor\n+PC559Bq7lz8vU0bWP39IeqYVKTakF9enlotEbkNw5qI7DNvXuXJPw4QZjN6fvsttm7dipcOHoT+\n+++xv3VrWHx8IG6aJV6s16NYknBWrU1BWAyFPABrgxNR3XJy5MlkTk5x0QPocvYsNu3ejQ0bNuDg\nwYP4f6dPo21QEIZdvow+QUFobjBAHxqK0k6dYBs9Gnemp0MsWABJyQxyFkMhD8EJZkRUtwULgFmz\nFC29MksS/tW+PX55+GHk5ubixIkT+Oyzz9CiRYsb67ErYg1ponIcBieiumVkKF4jbRQCz8bEYMmS\nJTh69Chee+01tGrVqvqgBoDwcJQOGgSbs0/IYijkQRjWRFS3/Hx12snLw88//4zs7Gw89NBDtd71\nwIEDeDozExa93rnn8tIa0uSZGNZEVLfAQHXaCQ7Ghx9+iOeeew76GkLYZrPh3XffxcCBA/HoW2/B\n/733WEOavB4nmBFR3aKjgXXrlA2F6/WwZGdjy48/YntWVrV3yc7OxtixY5GXl4effvoJkZGRN37o\nTCEWIg/BM2siqpsaO1ZZrZD++18cKihA8xdflNdtV5CWloauXbuiR48e2LFjR+Wgvr5HNuLi5Elj\nN2+vaDTK34+Lk+/HoCYPw9ngRGQfNWt7VzgDLho3DtOnT8f69euxevVq3H///bU/ljWkyQsxrInI\nPvVQ29tmMODtpk2RcffdWLFiBYJZwISoWgxrIrJfxdrgKrH4+cHn++8hle3ORURV8Jo1Edlv4kR5\n8pbJJA9lq8DXYoH0zjuqtEXkqXhmTUSO27NHrhWeliZ/rbBgCiuNEdWOZ9ZE5LgePeSlXKdPA/37\nA84WLikjSfKkMSKqFsOaiJwXFibPxrZalbVjNnPfaaJaMKyJSBkVS5ESUfUY1kSkjIqlSImoegxr\nIlImOlqeIKYE950mqhVngxORMtx3mqje8cyaiJQJDwcGD3Z+3TX3nSaqE8+siUg5JaVITSZ58w1u\nZ0lUI55ZE5FyMTE3Kps5gvtOE9mF+1kTkTrKtqXkvtNEquMwOBGpq2IpUkmSg7uM0SiH+JAhQGIi\nz6iJ7MSwJqL6wX2niVTDsCYiItI4TjAjIiLSOIY1ERGRxjGsiYiINI5hTUREpHEMayIiIo1jWBMR\nEWkcw5qIiEjjGNZEREQax7AmIiLSOIY1ERGRxjGsiYiINI5hTUREpHEMayIiIo1jWBMREWkcw5qI\niEjjGNZEREQax7AmIiLSOIY1ERGRxjGsiYiINI5hTUREpHEMayIiIo1jWBMREWkcw5qIiEjjGNZE\nREQax7AmIiLSOIY1ERGRxjGsiYiINI5hTUREpHEMayIiIo1jWBMREWkcw5qIiEjj/j+NVGPNhMZR\nqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "for e in edges:\n",
    "  G.add_edge(e[0],e[1])\n",
    "nx.draw(G)\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Connected Components:  3\n",
      "Largest Connected Component: 40\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Connected Components: \", nx.number_connected_components(G))\n",
    "print(\"Largest Connected Component:\", max([len(x) for x in nx.connected_components(G)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low IDs:   [697, 703, 708, 713, 719, 729, 745, 747, 753, 769, 772, 774, 798, 800, 803, 804, 805, 810, 811, 819]\n",
      "High IDs:  [823, 825, 828, 830, 840, 856, 861, 863, 864, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893]\n",
      "Normalized Cut Cost:  0.4224058769513316\n"
     ]
    }
   ],
   "source": [
    "largest_cc = sorted(max(nx.connected_components(G), key=len))\n",
    "\n",
    "half = round(len(largest_cc)/2)\n",
    "\n",
    "split_lo = largest_cc[:half]\n",
    "split_hi = largest_cc[half:]\n",
    "\n",
    "print(\"Low IDs:  \", split_lo)\n",
    "print(\"High IDs: \", split_hi)\n",
    "print(\"Normalized Cut Cost: \", (1/2)*nx.normalized_cut_size(G, split_lo, split_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current cost:  0.4224058769513316\n",
      "current cost:  0.1624924461156248\n",
      "current cost:  0.09817045961624274\n",
      "\n",
      "Number of Iterations:  3\n",
      "Cluster 1:  [729, 804, 825, 861, 863, 864, 876, 878, 882, 884, 886, 888, 889, 893]\n",
      "Cluster 2:  [697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 805, 810, 811, 819, 823, 828, 830, 840, 856, 869, 880, 890]\n",
      "Normalized Cut Cost:  0.09817045961624274\n"
     ]
    }
   ],
   "source": [
    "largest_cc = sorted(max(nx.connected_components(G), key=len))\n",
    "\n",
    "half = round(len(largest_cc)/2)\n",
    "\n",
    "split_lo = largest_cc[:half]\n",
    "split_hi = largest_cc[half:]\n",
    "\n",
    "prevCost = 0;\n",
    "currCost = (1/2)*nx.normalized_cut_size(G, split_lo, split_hi)\n",
    "\n",
    "iter = 0\n",
    "while prevCost != currCost:\n",
    "    iter += 1\n",
    "    prevCost = currCost\n",
    "    print(\"current cost: \", currCost)\n",
    "    lo = [i for i in split_lo] # deep copy\n",
    "    hi = [i for i in split_hi] # deep copy\n",
    "    \n",
    "    # move lo to hi if cut cost is lower\n",
    "    for val in split_lo:          \n",
    "        lo.remove(val)\n",
    "        hi.append(val)\n",
    "        newCost = (1/2)*nx.normalized_cut_size(G, lo, hi)\n",
    "        \n",
    "        if(newCost > currCost): # not lower, restore\n",
    "            lo.append(val)\n",
    "            hi.remove(val)\n",
    "        else:\n",
    "            currCost = newCost # update current cost\n",
    "\n",
    "    # move hi to lo if cut cost is lower\n",
    "    for val in split_hi:\n",
    "        lo.append(val)\n",
    "        hi.remove(val)\n",
    "        newCost = (1/2)*nx.normalized_cut_size(G, lo, hi)\n",
    "        \n",
    "        if(newCost > currCost): # not lower, restore\n",
    "            lo.remove(val)\n",
    "            hi.append(val)\n",
    "        else:\n",
    "            currCost = newCost # update current cost\n",
    "           \n",
    "    split_lo = [i for i in lo] # deep copy\n",
    "    split_hi = [i for i in hi] # deep copy\n",
    "    \n",
    "print(\"\\nNumber of Iterations: \", iter)\n",
    "print(\"Cluster 1: \", sorted(split_hi))\n",
    "print(\"Cluster 2: \", sorted(split_lo))\n",
    "print(\"Normalized Cut Cost: \", (1/2)*nx.normalized_cut_size(G, split_lo, split_hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def totalEdges(cluster):\n",
    "    count = 0\n",
    "    for i in range(0, len(cluster)):\n",
    "        for j in range(i, len(cluster)):\n",
    "            if(G.has_edge(cluster[i], cluster[j])):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "def totalAdjEdges(cluster1, cluster2):\n",
    "    count = 0\n",
    "    for i in range(0, len(cluster1)):\n",
    "        for j in range(0, len(cluster2)):\n",
    "            if(G.has_edge(cluster1[i], cluster2[j])):\n",
    "                count += 1\n",
    "    return count\n",
    "    \n",
    "def ekk(cluster, numEdges):\n",
    "    count = 0\n",
    "    for i in range(0, len(cluster)):\n",
    "        for j in range(i, len(cluster)):\n",
    "            if(G.has_edge(cluster[i], cluster[j])):\n",
    "                count += 1\n",
    "    return count/numEdges\n",
    "\n",
    "def ak(cluster, numEdges, numAdjEdges):\n",
    "    count = numAdjEdges\n",
    "    for i in range(0, len(cluster)):\n",
    "        for j in range(i, len(cluster)):\n",
    "            if(G.has_edge(cluster[i], cluster[j])):\n",
    "                count += 1\n",
    "    return count/numEdges\n",
    "\n",
    "def findQ(clusters, numEdges, numAdjEdges):\n",
    "    Q = 0\n",
    "    for c in clusters:\n",
    "        Q += ekk(c, numEdges) - ak(c, numEdges, numAdjEdges)*ak(c, numEdges, numAdjEdges)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current modularity:  -0.4288016528925619\n",
      "Current modularity:  -0.39396694214876027\n",
      "\n",
      "Number of Iterations:  2\n",
      "Cluster 1:  [697, 703, 713, 798, 828, 830, 840, 856, 869, 876, 878, 880, 882, 884, 886, 888, 889, 890, 893]\n",
      "Cluster 2:  [708, 719, 729, 745, 747, 753, 769, 772, 774, 800, 803, 804, 805, 810, 811, 819, 823, 825, 861, 863, 864]\n",
      "Max Modularity Cost:  0.35433884297520657\n"
     ]
    }
   ],
   "source": [
    "largest_cc = sorted(max(nx.connected_components(G), key=len))\n",
    "\n",
    "half = round(len(largest_cc)/2)\n",
    "\n",
    "split_lo = largest_cc[:half]\n",
    "split_hi = largest_cc[half:]\n",
    "\n",
    "numEdges = totalEdges(largest_cc)\n",
    "numAdjEdges = totalAdjEdges(split_lo, split_hi)\n",
    "\n",
    "prevQ = 0;\n",
    "currQ = findQ([split_lo, split_hi], numEdges, numAdjEdges)\n",
    "\n",
    "iter = 0\n",
    "while prevQ != currQ:\n",
    "    iter += 1\n",
    "    prevQ = currQ\n",
    "    numAdjEdges = totalAdjEdges(split_lo, split_hi)\n",
    "    \n",
    "    print(\"Current modularity: \", currQ)\n",
    "    lo = [i for i in split_lo] # deep copy\n",
    "    hi = [i for i in split_hi] # deep copy\n",
    "    \n",
    "    # move lo to hi if cut cost is lower\n",
    "    for val in split_lo:          \n",
    "        lo.remove(val)\n",
    "        hi.append(val)\n",
    "        newQ = findQ([lo, hi], numEdges, numAdjEdges)\n",
    "        \n",
    "        if(newQ < currQ): # lower, restore\n",
    "            lo.append(val)\n",
    "            hi.remove(val)\n",
    "        else:\n",
    "            currQ = newQ # update current cost\n",
    "\n",
    "    # move hi to lo if cut cost is lower\n",
    "    for val in split_hi:\n",
    "        lo.append(val)\n",
    "        hi.remove(val)\n",
    "        newQ = findQ([lo, hi], numEdges, numAdjEdges)\n",
    "\n",
    "        if(newQ < currQ): # lower, restore\n",
    "            lo.remove(val)\n",
    "            hi.append(val)\n",
    "        else:\n",
    "            currQ = newQ # update current cost\n",
    "           \n",
    "    split_lo = [i for i in lo] # deep copy\n",
    "    split_hi = [i for i in hi] # deep copy\n",
    "    \n",
    "print(\"\\nNumber of Iterations: \", iter)\n",
    "print(\"Cluster 1: \", sorted(split_hi))\n",
    "print(\"Cluster 2: \", sorted(split_lo))\n",
    "print(\"Max Modularity Cost: \", findQ([split_lo, split_hi], numEdges, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
